---
title: "Practical Machine Learning"
author: "Joel"
date: "May 24, 2018"
output: 
  html_document: 
    toc: yes
---


Data
====

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. 


Background
==========

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: 

exactly according to the specification (Class A)
throwing the elbows to the front (Class B)
lifting the dumbbell only halfway (Class C)
lowering the dumbbell only halfway (Class D) 
throwing the hips to the front (Class E).

Loading necessary packages
==========================

```{r}
# to model the data
library(caret)
# To visualize 
library(corrplot)
# parallel processing
library(doParallel)
```

Downloading the data
====================

```{r}
training <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), na.strings=c("NA","#DIV/0!",""))

testing <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), na.strings=c("NA","#DIV/0!",""))
```

Dimension of the data frame

```{r}

dim(training)
dim(testing)
```

Training data set has 160 variables and 19622 observations. 
Testing data set has 160 variables and 20 observations

```{r}
head(training)
```

We observe that a lot of columns have NAs. 


```{r}
head(training[,9:20])
```

We will remove the NAs as we find columns with more than 90% NA


```{r}
sum(sapply(training, function(x)sum(is.na(x))/length(x))>0.9)
```

excluding 100 variables out of 160

```{r}
training <- training[,!sapply(training, function(x) sum(is.na(x))/length(x))>0.9]
```

Repeating same on the test data

```{r}
testing <- testing[,!sapply(testing, function(x) sum(is.na(x))/length(x))>0.9]
```

removing unnecessary variables

```{r}
training <- training[,-(1:7)]
testing  <- testing [,-(1:7)]
```


Looking at the correlation of the variables

```{r}
corrVar <- cor(training[,1:52])
corrplot(corrVar,  type = "upper", tl.pos = "td",method = "circle", tl.cex = 0.5, tl.col = 'black',order = "hclust", diag = FALSE)

```

Finding columns with more than .75 correlation


```{r}
CorrCol <- findCorrelation(corrVar, cutoff=0.75)
length(CorrCol)
```
21 is the number of columns to be removed

Now, to remove correlated variables

```{r}
training <- training[,-CorrCol]
testing  <- testing[,-CorrCol]
```

Plotting it

```{r}
corrplot(cor(training[,1:30]),  type = "upper", tl.pos = "td",method = "circle", tl.cex = 0.5, tl.col = 'black',order = "hclust", diag = FALSE)
```

Modeling the dataset
====================

Ensuring the result can be replayed

```{r}
registerDoParallel()
set.seed(120)
```

Using k-fold cross validation

```{r}
TC <- trainControl(method = "cv", number = 10)
```

Using Random Forest Method

```{r}
RF <- train(classe~., data=training, trControl=TC, method="rf") 
RF
```

Using decision tree method to model data

```{r}
FitRP <- train(classe~., data=training, trControl=TC, method = "rpart")
FitRP 
```


As you can see above, the Random Forest prediction has a much higher accuracy than the Decision Tree accuracy. Therefore we will use the Random Forest prediction to predict our test cases.

The accuracy of the model is 0.99. The expected out-of-sample error is estimated at 0.005, or 0.5%. The expected out-of-sample error is calculated as 1 - accuracy for predictions made against the cross-validation set. Our Test data set comprises 20 cases. With an accuracy above 99% on our cross-validation data, we can expect that very few, or none, of the test samples will be missclassified.

The prediction of our algorithm for the test set is:

```{r}
predictingrf <- predict(RF,testing)
predictingrf
```


We then save the output to files according to instructions and post it to the submission page.

```{r}
FileWrite = function(x){
    n = length(x)
    path <- "./PA"
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=file.path(path, filename),quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

FileWrite(predictingrf)
```
